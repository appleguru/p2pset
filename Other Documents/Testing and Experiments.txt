	To test how well our application scales, we will write a testing client (or add a testing functionality to our main app) that will spawn an increasing number of clients and claim sets. Each client will time how long it takes from when it releases the critical section token to when it receives it again. This will get plotted against the number of clients. The expected outcome of this test is that it will take an increasing amount of time for the critical section to return to us as we add more clients.
	The clients will also record whenever their set claims are invalid (because someone else has already claimed the critical section and claimed a set). This too will get plotted against the number of clients. We will do this  in two scenarios, in one with each client claiming sets as fast as possible, and in another with a randomly timed sleep (between 0 and 10 seconds, for example) that simulates a human's processing time. With the human simulation, we can use the data we gather to establish a maximum number of clients allowed in a game based on an gameplay a set claim failure threshold that we deem acceptable.
	These experiments will allow us to evaluate how effective our distributed mutex is as the number of simultaneous clients increases and allow us to come to preliminary conclusions about game limitations based on the practical application of our algorithms.
	In addition to testing how well our application scales, we will implement a testing routine that tests the state reconciliation function of our application. It will spawn off clients that claim sets. Some clients will then improperly disconnect when they have the critical section. The game state will then be checked and confirmed amongst all remaining clients. We will keep track of the state of the client that dropped and plot how often the majority determines the correct game state.